{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d466657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_anonymizer.entities import OperatorConfig\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee1094e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip (no docx/doc found) → Starry\n",
      "Skip (no docx/doc found) → Atheon\n",
      "Skip (no docx/doc found) → Slickdeals\n",
      "Skip (no docx/doc found) → Procure\n",
      "Skip (no docx/doc found) → General Mills\n",
      "Skip (no docx/doc found) → Preqin\n",
      "Skip (no docx/doc found) → Ocean Spray\n",
      "Skip (no docx/doc found) → Tipalti\n",
      "Skip (no docx/doc found) → Workato\n",
      "Skip (no docx/doc found) → Digitalizar\n",
      "Skip (no docx/doc found) → FDA\n",
      "Skip (no docx/doc found) → Coffee and Bagel\n",
      "Skip (no docx/doc found) → Risk Strategies\n",
      "Skip (no docx/doc found) → HDI\n",
      "Skip (no docx/doc found) → HDSupply\n",
      "Skip (no docx/doc found) → Boxed\n",
      "Skip (no docx/doc found) → CIC\n",
      "Skip (no docx/doc found) → Trumid\n",
      "Skip (no docx/doc found) → FunPlus\n",
      "Skip (no docx/doc found) → QCommissions\n",
      "Skip (no docx/doc found) → Next Games\n",
      "Skip (no docx/doc found) → Disney Ad Sales\n",
      "Skip (no docx/doc found) → Performio\n",
      "Skip (no docx/doc found) → Crocs\n",
      "Skip (no docx/doc found) → Tessera\n",
      "Skip (no docx/doc found) → SurveyMonkey\n",
      "Skip (no docx/doc found) → JD Power\n",
      "Skip (no docx/doc found) → IRI\n",
      "Skip (no docx/doc found) → Blueshift\n",
      "Skip (no docx/doc found) → Care Health Insurance\n",
      "Skip (no docx/doc found) → Varicent\n",
      "Skip (no docx/doc found) → RBC\n",
      "Skip (no docx/doc found) → Avnet\n",
      "Skip (no docx/doc found) → Metadata\n",
      "Skip (no docx/doc found) → Commonwealth of VA\n",
      "Skip (no docx/doc found) → Diamanti\n",
      "Skip (no docx/doc found) → Ross\n",
      "Copied 491 files to 'input_ndas'\n"
     ]
    }
   ],
   "source": [
    "def collect_latest_docx_files(source_dir: Path, dest_dir: Path):\n",
    "    dest_dir.mkdir(exist_ok=True)\n",
    "    folder_names = []\n",
    "\n",
    "    for item in source_dir.iterdir():\n",
    "        if item.is_dir():\n",
    "            folder_name = item.name\n",
    "            if folder_name in [\"Tietievry\", \"Investors Bank\", \"Old Republic\", \"ThoughSpot Partnerships\", \"PayPal\", \"Quantum Health\"]:\n",
    "                continue\n",
    "            docx_files = list(item.glob(\"*.docx\")) + list(item.glob(\"*.doc\"))\n",
    "            if not docx_files:\n",
    "                print(f\"Skip (no docx/doc found) → {folder_name}\")\n",
    "                continue\n",
    "\n",
    "            folder_names.append(folder_name)\n",
    "\n",
    "            try:\n",
    "                latest_file = max(\n",
    "                    docx_files,\n",
    "                    key=lambda f: (f.stat().st_mtime, f.stat().st_size)\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Skip (error getting latest file) → {folder_name}: {e}\")\n",
    "                continue\n",
    "            new_filename = f\"{folder_name}_{latest_file.name}\"\n",
    "            destination_path = dest_dir / new_filename\n",
    "            shutil.copy2(latest_file, destination_path)\n",
    "\n",
    "    return folder_names\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    source_folder = Path(\"NDA DATASET 2023 2024 DG\")\n",
    "    destination_folder = Path(\"input_ndas\")\n",
    "    \n",
    "    ORG_PATTERNS = [\n",
    "        r'\\binc\\b',\n",
    "        r'\\bcorporation\\b',\n",
    "        r'\\bllc\\b',\n",
    "        r'\\bcompany\\b'\n",
    "    ]\n",
    "    \n",
    "    processed_folders = collect_latest_docx_files(source_folder, destination_folder)\n",
    "    print(f\"Copied {len(processed_folders)} files to '{destination_folder}'\")    \n",
    "    for folder_name in processed_folders:\n",
    "        clean_name = folder_name.lower()\n",
    "        escaped_name = re.escape(clean_name)\n",
    "        regex_pattern = fr'\\b{escaped_name}\\b'\n",
    "        ORG_PATTERNS.append(regex_pattern)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ce665f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing NDA files: 100%|██████████████████████████████████████████| 491/491 [03:20<00:00,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "skip 0 file that cannot open\n",
      "skip 241 no ending words in file\n",
      "Processing 250 files in total.\n",
      "\n",
      "Output file has been saved to: ./parsed_nda_para.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from docx import Document\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_anonymizer.entities import OperatorConfig\n",
    "\n",
    "ORG_PATTERNS = [\n",
    "    r'\\bpaypal\\b',\n",
    "    r'\\bbayer\\b',\n",
    "    r'\\bpricenow\\b',\n",
    "    r'\\bthoughtspot\\b',\n",
    "    r'\\binc\\b',\n",
    "    r'\\bcorporation\\b',\n",
    "    r'\\bllc\\b',\n",
    "    r'\\bcompany\\b'\n",
    "]\n",
    "\n",
    "ENTITIES_TO_REDACT = [\n",
    "    \"PERSON\", \"ORGANIZATION\", \"EMAIL_ADDRESS\", \"PHONE_NUMBER\", \"NRP\"\n",
    "]\n",
    "\n",
    "def pre_clean_orgs(text):\n",
    "    clean_text = text\n",
    "    for pat in ORG_PATTERNS:\n",
    "        clean_text = re.sub(pat, \"[REDACTED_ORG]\", clean_text, flags=re.IGNORECASE)\n",
    "    return clean_text\n",
    "\n",
    "def redact_text(text):\n",
    "    if not text.strip():\n",
    "        return \"\"\n",
    "    text = pre_clean_orgs(text)\n",
    "    results = analyzer.analyze(\n",
    "        text=text,\n",
    "        language='en',\n",
    "        entities=ENTITIES_TO_REDACT\n",
    "    )\n",
    "    anonymized = anonymizer.anonymize(\n",
    "        text=text,\n",
    "        analyzer_results=results,\n",
    "        operators={\"DEFAULT\": OperatorConfig(\"replace\", {\"new_value\": \"[REDACTED]\"})}\n",
    "    )\n",
    "    return anonymized.text\n",
    "\n",
    "\n",
    "\n",
    "def parse_nda(doc_path, cant_open_doc_num, redact=True):\n",
    "    try:\n",
    "        doc = Document(doc_path)\n",
    "    except Exception as e:\n",
    "        # print(f\"Skipped (cannot open as Word): {os.path.basename(doc_path)} | {e}\")\n",
    "        cant_open_doc_num +=1\n",
    "        return None\n",
    "\n",
    "    all_text_body = []\n",
    "    found_in_body = False\n",
    "\n",
    "    END_PATTERNS = [\n",
    "        r'in\\s+witness\\s+whereof',\n",
    "        r'in\\s+witness\\s+thereof',\n",
    "        r'the\\s+signatures\\s+follow',\n",
    "        r'signature\\s+page\\s+follows',\n",
    "        r'signed\\s+on\\s+behalf\\s+of',\n",
    "        r'signed\\s+for\\s+and\\s+on\\s+behalf\\s+of',\n",
    "        r'each\\s+acting\\s+under\\s+due\\s+and\\s+proper\\s+authority'\n",
    "    ]\n",
    "    end_pattern = re.compile('(' + '|'.join(END_PATTERNS) + ')', re.IGNORECASE)\n",
    "\n",
    "    for para in doc.paragraphs:\n",
    "        text = para.text.strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        all_text_body.append(text)  \n",
    "        if end_pattern.search(text): \n",
    "            found_in_body = True\n",
    "            break\n",
    "\n",
    "    if not found_in_body:\n",
    "        if not doc.tables or len(doc.tables) == 0:\n",
    "            return None\n",
    "        found_in_table = False\n",
    "        for table in doc.tables:\n",
    "            for row in table.rows:\n",
    "                for cell in row.cells:\n",
    "                    cell_text = cell.text.strip()\n",
    "                    if not cell_text:\n",
    "                        continue\n",
    "                    if end_pattern.search(cell_text):\n",
    "                        found_in_table = True\n",
    "                        break\n",
    "                if found_in_table:\n",
    "                    break\n",
    "            if found_in_table:\n",
    "                break\n",
    "        if not found_in_table:\n",
    "            return None\n",
    "\n",
    "    raw_sentences = []\n",
    "    raw_para_map = [] \n",
    "    for para_text in all_text_body:\n",
    "        split_sents = re.split(r'(?<=[.?!])\\s+(?=[A-Z(])', para_text)\n",
    "        for s in split_sents:\n",
    "            s = s.strip()\n",
    "            if s:\n",
    "                raw_sentences.append(s)\n",
    "                raw_para_map.append(para_text) \n",
    "\n",
    "    parsed_sentences = []\n",
    "    parsed_para_map = []\n",
    "    main_clause = None\n",
    "\n",
    "    for sent, para in zip(raw_sentences, raw_para_map):\n",
    "        sent = sent.strip()\n",
    "        if not sent:\n",
    "            continue\n",
    "        if len(sent.split()) < 10 and sent.endswith(\".\"):\n",
    "            continue\n",
    "        elif sent.endswith(\":\"):\n",
    "            main_clause = sent\n",
    "        elif main_clause:\n",
    "            parsed_sentences.append(f\"{main_clause} {sent}\")\n",
    "            parsed_para_map.append(para)\n",
    "            if sent.endswith(\".\"):\n",
    "                main_clause = None\n",
    "        else:\n",
    "            parsed_sentences.append(sent)\n",
    "            parsed_para_map.append(para)\n",
    "\n",
    "    def normalize_text(text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'^[a-z]\\)|^\\([a-z]\\)|^\\d+[\\.\\)]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "\n",
    "    normalized_sentences = [normalize_text(s) for s in parsed_sentences]\n",
    "\n",
    "    if redact:\n",
    "        clean_sentences = [redact_text(s) for s in normalized_sentences]\n",
    "        clean_paragraphs = [redact_text(normalize_text(p)) for p in parsed_para_map]\n",
    "    else:\n",
    "        clean_sentences = normalized_sentences\n",
    "        clean_paragraphs = [normalize_text(p) for p in parsed_para_map]\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"original_sentence\": parsed_sentences,\n",
    "        \"clean_sentence\": clean_sentences,\n",
    "        \"clean_paragraph\": clean_paragraphs  \n",
    "    })\n",
    "\n",
    "    df = df[\n",
    "        ~((df[\"original_sentence\"].apply(lambda x: len(x.split()) <= 5)) &\n",
    "          (df[\"clean_sentence\"].apply(lambda x: len(x.split()) <= 5)))\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"./input_ndas\"\n",
    "    all_results = []\n",
    "\n",
    "    analyzer = AnalyzerEngine()\n",
    "    anonymizer = AnonymizerEngine()\n",
    "\n",
    "    docx_files = [f for f in os.listdir(folder_path) if f.lower().endswith((\".docx\", \".doc\"))]\n",
    "    global cant_open_doc_num\n",
    "    cant_open_doc_num = 0\n",
    "    no_ending_num = 0\n",
    "    for filename in tqdm(docx_files, desc=\"Parsing NDA files\", ncols=100):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        parsed_df = parse_nda(file_path,cant_open_doc_num,redact=True)\n",
    "\n",
    "        if parsed_df is not None and not parsed_df.empty:\n",
    "            parsed_df[\"source_file\"] = filename\n",
    "            all_results.append(parsed_df)\n",
    "        else:\n",
    "            no_ending_num+=1\n",
    "        #     print(f\"Skipped (no ending words found in body or tables): {filename}\")\n",
    "\n",
    "    print()\n",
    "    print(f\"skip {cant_open_doc_num} file that cannot open\")\n",
    "    print(f\"skip {no_ending_num} no ending words in file\")\n",
    "    print(f\"Processing {len(all_results)} files in total.\")\n",
    "\n",
    "    final_df = pd.concat(all_results, ignore_index=True)\n",
    "    output_path = \"./parsed_nda_para.csv\"\n",
    "    final_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nOutput file has been saved to: {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade7d99f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
